{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfS6VdJMIDgi",
        "outputId": "df7d8d2d-edc3-4e34-97a2-1a41a229bc5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZxpYz8NIJ2Q",
        "outputId": "46fde75b-dd38-4be6-df9a-a7b2ab2c1a73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n"
      ],
      "metadata": {
        "id": "hxf5pm2GlmmQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data processing\n"
      ],
      "metadata": {
        "id": "OnB4p0aFl1Wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Reading the dataset***"
      ],
      "metadata": {
        "id": "t4WB77ofnpBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the csv\n",
        "df = pd.read_csv('/content/drive/MyDrive/TrabajoFinal/cyberbullying_tweets.csv', names=['text', 'cyberbullying_type'])\n",
        "\n",
        "\n",
        "total_nulls = df.isnull().sum().sum()\n",
        "\n",
        "print(\"Null data:\", total_nulls)\n",
        "\n",
        "# Making sure is a string (just in case)\n",
        "df['cyberbullying_type'] = df['cyberbullying_type'].astype(str)\n",
        "\n",
        "# Select 200 text for each label\n",
        "df_sampled = df.groupby('cyberbullying_type').apply(lambda x: x.sample(n=min(len(x), 300), random_state=42)).reset_index(drop=True)\n",
        "\n",
        "# Drop the categories we don't plan to use\n",
        "df_sampled = df_sampled[~df_sampled['cyberbullying_type'].isin(['cyberbullying_type', 'other_cyberbullying'])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVisC-DZmZeO",
        "outputId": "c5731fa4-0793-4050-cbc1-74c981934d9f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null data: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Getting the labels***"
      ],
      "metadata": {
        "id": "Bgq75qaEnszG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to categorical and set a index number for the selected labels\n",
        "df_sampled['cyberbullying_type'] = df_sampled['cyberbullying_type'].astype('category')\n",
        "df_sampled['label'] = df_sampled['cyberbullying_type'].cat.codes\n",
        "\n",
        "# Crear el mapeo de categorías a códigos numéricos\n",
        "category_mapping = dict(enumerate(df_sampled['cyberbullying_type'].cat.categories))\n",
        "\n",
        "print(\"Ciberbullying numerical labels:\")\n",
        "for code, category in category_mapping.items():\n",
        "    print(f\"{code}: {category}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQxTFJTUnnZ2",
        "outputId": "1aaec38d-33b2-4378-bf7b-0f7fe3b15603"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciberbullying numerical labels:\n",
            "0: age\n",
            "1: ethnicity\n",
            "2: gender\n",
            "3: not_cyberbullying\n",
            "4: religion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Getting the train and test values***"
      ],
      "metadata": {
        "id": "HIS_vLr8n8eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "train_df, test_df = train_test_split(df_sampled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "b70_mYHmn7yY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tokenization using GPT-2***"
      ],
      "metadata": {
        "id": "wvkugwOAoY3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-2 Tokenizator\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token = '[PAD]'"
      ],
      "metadata": {
        "id": "ITH6p2jBosPi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Preparing the data in order to train and validate***"
      ],
      "metadata": {
        "id": "-Tw7AfeGpIl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CyberbullyingDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].flatten()\n",
        "        attention_mask = encoding['attention_mask'].flatten()\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ScsXCXsBo0ev"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Calling the class we made***"
      ],
      "metadata": {
        "id": "9ciSwoNqpeck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the datasets for the training and tests\n",
        "train_dataset = CyberbullyingDataset(\n",
        "    texts=train_df['text'].tolist(),\n",
        "    labels=train_df['label'].tolist(),\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = CyberbullyingDataset(\n",
        "    texts=test_df['text'].tolist(),\n",
        "    labels=test_df['label'].tolist(),\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "BDlSWlBjpeQA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Instantiating the model***"
      ],
      "metadata": {
        "id": "v7gxmL7AqCUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo GPT-2 para clasificación de secuencias y ajustar los tokens especiales\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=df_sampled['cyberbullying_type'].nunique())\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Establecer el token de padding en la configuración del modelo\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Mover el modelo al mismo dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "ib5G79IsqNee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398e66ee-cf9d-4246-fcf2-02af2bf7c4ba"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50258, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Defining and training the model***"
      ],
      "metadata": {
        "id": "gy948psmria2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir los argumentos de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Directorio de salida para los checkpoints y los logs\n",
        "    num_train_epochs=5,              # Número total de epochs de entrenamiento\n",
        "    per_device_train_batch_size=8,   # Tamaño del batch de entrenamiento por dispositivo\n",
        "    per_device_eval_batch_size=8,    # Tamaño del batch de evaluación por dispositivo\n",
        "    warmup_steps=500,                # Número de pasos para el calentamiento del optimizador\n",
        "    weight_decay=0.01,               # Factor de decaimiento para el ajuste de los pesos\n",
        "    logging_dir='./logs',            # Directorio de logs\n",
        "    evaluation_strategy='epoch'      # Evaluar el modelo al final de cada epoch\n",
        ")\n",
        "\n",
        "\n",
        "# Crear el objeto Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "dbu4BlPgrrsb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "b3cb62ab-dd97-4a57-8d6c-3362b4c5b403"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 03:12, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.320223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.398461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.388760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.099100</td>\n",
              "      <td>0.341385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.099100</td>\n",
              "      <td>0.435851</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=750, training_loss=0.7674825337727864, metrics={'train_runtime': 192.7859, 'train_samples_per_second': 31.123, 'train_steps_per_second': 3.89, 'total_flos': 391955742720000.0, 'train_loss': 0.7674825337727864, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluation and proccess to save the trained model***"
      ],
      "metadata": {
        "id": "IimB2Pg9r1kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo en el conjunto de prueba\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_classes = np.argmax(predictions.predictions, axis=1)\n",
        "true_classes = test_df['label'].values\n",
        "\n",
        "# Mostrar métricas de clasificación\n",
        "print(classification_report(true_classes, predicted_classes))\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "model.save_pretrained('./gpt2_cyberbullying_model')\n",
        "tokenizer.save_pretrained('./gpt2_cyberbullying_tokenizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "y_h5lT-HILla",
        "outputId": "7cca3954-35df-4e55-dba2-cc0d24269122"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95        56\n",
            "           1       0.98      0.94      0.96        65\n",
            "           2       0.80      0.92      0.86        49\n",
            "           3       0.86      0.77      0.82        66\n",
            "           4       0.91      0.95      0.93        64\n",
            "\n",
            "    accuracy                           0.90       300\n",
            "   macro avg       0.90      0.91      0.90       300\n",
            "weighted avg       0.91      0.90      0.90       300\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2_cyberbullying_tokenizer/tokenizer_config.json',\n",
              " './gpt2_cyberbullying_tokenizer/special_tokens_map.json',\n",
              " './gpt2_cyberbullying_tokenizer/vocab.json',\n",
              " './gpt2_cyberbullying_tokenizer/merges.txt',\n",
              " './gpt2_cyberbullying_tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Using the model in inference***"
      ],
      "metadata": {
        "id": "SOf3yqt2sCFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacer inferencias con el modelo entrenado\n",
        "text_to_predict = \"I was bullied for not having a gf throughout my high school years. Even got called gay and what not. Joke was on them when I showed up to my senior prom with a college girl\"\n",
        "encoded_text = tokenizer(text_to_predict, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Mover los tensores a la misma GPU que el modelo si es necesario\n",
        "encoded_text = {k: v.to(device) for k, v in encoded_text.items()}\n",
        "\n",
        "logits = model(**encoded_text).logits\n",
        "predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "# Mostrar la clase predicha junto con su descripción\n",
        "predicted_category = category_mapping[predicted_class]\n",
        "print(\"Clase predicha:\", predicted_class, \"-\", predicted_category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWvU832VYN0R",
        "outputId": "0472e586-7e2b-4641-ebb1-21cb230fbd50"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase predicha: 0 - age\n"
          ]
        }
      ]
    }
  ]
}